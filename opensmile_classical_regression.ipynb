{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive, pull data from csv processed in stressid_split.ipynb and train baseline models\n"
      ],
      "metadata": {
        "id": "l1XI5MiqmieH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JTrAVkHkHIB",
        "outputId": "185ffe94-f41d-4236-c1d3-b097a8366e1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Stress"
      ],
      "metadata": {
        "id": "MyKnDPmCm-GZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28fd111b",
        "outputId": "b1fb9104-6e3e-4d84-d1cd-f2965c002681"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 3. Define paths\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\"\n",
        "EXTRACTED_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\"\n",
        "\n",
        "# 4. Load CSVs\n",
        "print(\"Loading datasets...\")\n",
        "traindf = pd.read_csv(TRAIN_PATH)\n",
        "testdf = pd.read_csv(TEST_PATH)\n",
        "extracteddf = pd.read_csv(EXTRACTED_PATH)\n",
        "\n",
        "print(f\"Train split rows: {len(traindf)}\")\n",
        "print(f\"Test split rows: {len(testdf)}\")\n",
        "print(f\"Features rows: {len(extracteddf)}\")\n",
        "\n",
        "# 5. Merge Features with Splits\n",
        "# Train/Test have 'subject/task', Features have 'basename'. We merge on these.\n",
        "print(\"Merging datasets...\")\n",
        "train_merged = pd.merge(traindf, extracteddf, left_on='subject/task', right_on='basename', how='inner')\n",
        "test_merged = pd.merge(testdf, extracteddf, left_on='subject/task', right_on='basename', how='inner')\n",
        "\n",
        "print(f\"Merged Train Shape: {train_merged.shape}\")\n",
        "print(f\"Merged Test Shape: {test_merged.shape}\")\n",
        "\n",
        "# 6. Prepare X and y\n",
        "# Identify target column (handle potential naming variations)\n",
        "TARGET_COL = \"binary-stress\"\n",
        "if TARGET_COL not in train_merged.columns:\n",
        "    if \"binary_stress\" in train_merged.columns:\n",
        "        TARGET_COL = \"binary_stress\"\n",
        "    else:\n",
        "        raise ValueError(f\"Target column not found. Available columns: {train_merged.columns}\")\n",
        "\n",
        "y_train = train_merged[TARGET_COL].astype(int)\n",
        "y_test = test_merged[TARGET_COL].astype(int)\n",
        "\n",
        "# Identify feature columns: all numeric columns in merged df excluding metadata\n",
        "metadata_cols = [\n",
        "    'subject/task', 'binary-stress', 'binary_stress', 'affect3-class', 'subject', 'isuseful', 'path', 'continuouslabel',\n",
        "    'file', 'basename', 'filename', 'audio_path', 'label', 'class', 'id', 'Unnamed: 0'\n",
        "]\n",
        "\n",
        "feature_cols = [c for c in train_merged.columns if c not in metadata_cols and pd.api.types.is_numeric_dtype(train_merged[c])]\n",
        "\n",
        "X_train_raw = train_merged[feature_cols]\n",
        "X_test_raw = test_merged[feature_cols]\n",
        "\n",
        "# 7. Clean and Preprocess\n",
        "# Handle missing values (impute with median from train)\n",
        "imputer = X_train_raw.median(numeric_only=True)\n",
        "X_train = X_train_raw.fillna(imputer).fillna(0) # fill remaining NaNs with 0\n",
        "X_test = X_test_raw.fillna(imputer).fillna(0)\n",
        "\n",
        "# Variance Threshold (remove constant features)\n",
        "vt = VarianceThreshold(threshold=0.0)\n",
        "X_train_v = vt.fit_transform(X_train)\n",
        "X_test_v = vt.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_train = pd.DataFrame(X_train_v, columns=X_train.columns[vt.get_support()], index=y_train.index)\n",
        "X_test = pd.DataFrame(X_test_v, columns=X_train.columns, index=y_test.index)\n",
        "\n",
        "# 8. Setup variables for downstream compatibility (CV groups, etc.)\n",
        "df = train_merged.copy()\n",
        "train_idx = df.index # Since df is exactly the training set now\n",
        "\n",
        "print(f\"Final X_train shape: {X_train.shape}\")\n",
        "print(f\"Final X_test shape: {X_test.shape}\")\n",
        "print(f\"Class distribution in Train:\\n{y_train.value_counts()}\")\n",
        "print(f\"Class distribution in Test:\\n{y_test.value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Train split rows: 440\n",
            "Test split rows: 70\n",
            "Features rows: 371\n",
            "Merging datasets...\n",
            "Merged Train Shape: (440, 46)\n",
            "Merged Test Shape: (70, 46)\n",
            "Final X_train shape: (440, 37)\n",
            "Final X_test shape: (70, 37)\n",
            "Class distribution in Train:\n",
            "binary-stress\n",
            "0    220\n",
            "1    220\n",
            "Name: count, dtype: int64\n",
            "Class distribution in Test:\n",
            "binary-stress\n",
            "1    41\n",
            "0    29\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === StressID (binary-stress) model training using pre-extracted openSMILE features ===\n",
        "# Models: SVM (RBF, tuned), Gradient Boosting (tuned), SVC (baseline), Logistic Regression, Random Forest\n",
        "# Assumes Google Drive is already mounted.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Paths (uses your Drive paths; includes fallback if you uploaded to runtime)\n",
        "# -----------------------------\n",
        "def pick_path(*candidates):\n",
        "    for p in candidates:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of the provided paths exist:\\n\" + \"\\n\".join(candidates))\n",
        "\n",
        "OPENSMILE_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\",\n",
        "    \"/mnt/data/extracted_opensmile_features.csv\",\n",
        ")\n",
        "\n",
        "TRAIN_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\",\n",
        "    \"/mnt/data/stressidtrainbalanced2.csv\",\n",
        ")\n",
        "\n",
        "TEST_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\",\n",
        "    \"/mnt/data/stressidtest2.csv\",\n",
        ")\n",
        "\n",
        "print(\"Using:\")\n",
        "print(\" openSMILE:\", OPENSMILE_PATH)\n",
        "print(\" train:\", TRAIN_SPLIT_PATH)\n",
        "print(\" test :\", TEST_SPLIT_PATH)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load CSVs + join on path\n",
        "# -----------------------------\n",
        "opensmile = pd.read_csv(OPENSMILE_PATH)\n",
        "train_df = pd.read_csv(TRAIN_SPLIT_PATH)\n",
        "test_df  = pd.read_csv(TEST_SPLIT_PATH)\n",
        "\n",
        "def norm_path(series: pd.Series) -> pd.Series:\n",
        "    return (series.astype(str)\n",
        "            .str.replace(\"\\\\\", \"/\", regex=False)\n",
        "            .str.strip())\n",
        "\n",
        "# Keys to merge on (paths should match between split files and openSMILE file)\n",
        "opensmile[\"path_key\"] = norm_path(opensmile[\"file\"])\n",
        "train_df[\"path_key\"]  = norm_path(train_df[\"path\"])\n",
        "test_df[\"path_key\"]   = norm_path(test_df[\"path\"])\n",
        "\n",
        "# Merge: keep split rows (train/test), attach features\n",
        "opensmile_feats = opensmile.drop(columns=[\"file\"])  # keep basename + feature columns + path_key\n",
        "train_merged = train_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "test_merged  = test_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "\n",
        "# Sanity checks\n",
        "missing_train = train_merged.isna().any(axis=1).sum()\n",
        "missing_test  = test_merged.isna().any(axis=1).sum()\n",
        "print(f\"Rows with any missing values after merge -> train: {missing_train}, test: {missing_test}\")\n",
        "\n",
        "if train_merged.filter(like=\"F0\").shape[1] == 0:\n",
        "    print(\"WARNING: It looks like no openSMILE feature columns were merged. Check that 'path' matches openSMILE 'file'.\")\n",
        "\n",
        "# Features = numeric columns from openSMILE (exclude identifiers/labels)\n",
        "exclude_cols = {\n",
        "    \"subject/task\", \"binary-stress\", \"affect3-class\", \"subject\", \"isuseful\", \"path\", \"continuouslabel\",\n",
        "    \"basename\", \"path_key\"\n",
        "}\n",
        "feature_cols = [c for c in train_merged.columns if c not in exclude_cols]\n",
        "feature_cols = train_merged[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "LABEL_COL = \"binary-stress\"\n",
        "\n",
        "X_train = train_merged[feature_cols]\n",
        "y_train = train_merged[LABEL_COL].astype(int)\n",
        "\n",
        "X_test = test_merged[feature_cols]\n",
        "y_test = test_merged[LABEL_COL].astype(int)\n",
        "\n",
        "# Group key to avoid leakage from duplicated/oversampled rows:\n",
        "# (your balanced train file likely repeats the same path multiple times)\n",
        "groups_train = train_merged[\"path_key\"]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
        "print(\"Train label distribution:\\n\", y_train.value_counts().to_string())\n",
        "print(\"Test  label distribution:\\n\", y_test.value_counts().to_string())\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Define models\n",
        "# -----------------------------\n",
        "# StratifiedGroupKFold prevents duplicates (same path_key) from landing in both train/val during tuning\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "models = {}\n",
        "\n",
        "# SVC baseline (RBF defaults)\n",
        "models[\"SVC (Baseline)\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", random_state=42)),  # probability=False (faster); use decision_function for AUC\n",
        "])\n",
        "\n",
        "# Logistic Regression\n",
        "models[\"Logistic Regression\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=3000, solver=\"liblinear\", random_state=42)),\n",
        "])\n",
        "\n",
        "# Random Forest\n",
        "models[\"Random Forest\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Tuned SVM (RBF)\n",
        "svm_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", random_state=42)),\n",
        "])\n",
        "svm_grid = {\n",
        "    \"clf__C\": [0.1, 1, 10],\n",
        "    \"clf__gamma\": [\"scale\", 0.01, 0.1],\n",
        "}\n",
        "models[\"SVM (RBF, Tuned)\"] = GridSearchCV(\n",
        "    estimator=svm_pipe,\n",
        "    param_grid=svm_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "# Tuned Gradient Boosting\n",
        "gb_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", GradientBoostingClassifier(random_state=42)),\n",
        "])\n",
        "gb_grid = {\n",
        "    \"clf__n_estimators\": [100, 200],\n",
        "    \"clf__learning_rate\": [0.05, 0.1],\n",
        "    \"clf__max_depth\": [2, 3],\n",
        "    \"clf__subsample\": [1.0, 0.8],\n",
        "}\n",
        "models[\"Gradient Boosting (Tuned)\"] = GridSearchCV(\n",
        "    estimator=gb_pipe,\n",
        "    param_grid=gb_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Train + evaluate\n",
        "# -----------------------------\n",
        "def get_auc_scores(model, X):\n",
        "    # Prefer proba if available, else decision_function (works for SVC)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:, 1]\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        return model.decision_function(X)\n",
        "    return None\n",
        "\n",
        "rows = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Fit (pass groups ONLY for GridSearchCV using StratifiedGroupKFold)\n",
        "    if isinstance(model, GridSearchCV):\n",
        "        model.fit(X_train, y_train, groups=groups_train)\n",
        "        best_params = model.best_params_\n",
        "        fitted = model.best_estimator_\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_params = {}\n",
        "        fitted = model\n",
        "\n",
        "    y_pred = fitted.predict(X_test)\n",
        "    y_score = get_auc_scores(fitted, X_test)\n",
        "\n",
        "    row = {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"F1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"ROC_AUC\": roc_auc_score(y_test, y_score) if y_score is not None else np.nan,\n",
        "        \"Best Params (if tuned)\": best_params,\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "results = pd.DataFrame(rows).sort_values(\"F1\", ascending=False)\n",
        "\n",
        "# Pretty display\n",
        "display(results.style.format({\n",
        "    \"Accuracy\": \"{:.4f}\",\n",
        "    \"Precision\": \"{:.4f}\",\n",
        "    \"Recall\": \"{:.4f}\",\n",
        "    \"F1\": \"{:.4f}\",\n",
        "    \"ROC_AUC\": \"{:.4f}\",\n",
        "}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "XDQWML7M0ZhB",
        "outputId": "e3f396eb-fc10-48ca-ffac-8d7f1ebf186d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            " openSMILE: /content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\n",
            " train: /content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\n",
            " test : /content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\n",
            "Rows with any missing values after merge -> train: 0, test: 0\n",
            "Train shape: (440, 37)  Test shape: (70, 37)\n",
            "Train label distribution:\n",
            " binary-stress\n",
            "0    220\n",
            "1    220\n",
            "Test  label distribution:\n",
            " binary-stress\n",
            "1    41\n",
            "0    29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x788d3f0c9730>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_b5441\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_b5441_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_b5441_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_b5441_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
              "      <th id=\"T_b5441_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_b5441_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
              "      <th id=\"T_b5441_level0_col5\" class=\"col_heading level0 col5\" >ROC_AUC</th>\n",
              "      <th id=\"T_b5441_level0_col6\" class=\"col_heading level0 col6\" >Best Params (if tuned)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_b5441_level0_row0\" class=\"row_heading level0 row0\" >4</th>\n",
              "      <td id=\"T_b5441_row0_col0\" class=\"data row0 col0\" >Gradient Boosting (Tuned)</td>\n",
              "      <td id=\"T_b5441_row0_col1\" class=\"data row0 col1\" >0.6571</td>\n",
              "      <td id=\"T_b5441_row0_col2\" class=\"data row0 col2\" >0.6441</td>\n",
              "      <td id=\"T_b5441_row0_col3\" class=\"data row0 col3\" >0.9268</td>\n",
              "      <td id=\"T_b5441_row0_col4\" class=\"data row0 col4\" >0.7600</td>\n",
              "      <td id=\"T_b5441_row0_col5\" class=\"data row0 col5\" >0.6081</td>\n",
              "      <td id=\"T_b5441_row0_col6\" class=\"data row0 col6\" >{'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__n_estimators': 100, 'clf__subsample': 0.8}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b5441_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_b5441_row1_col0\" class=\"data row1 col0\" >Random Forest</td>\n",
              "      <td id=\"T_b5441_row1_col1\" class=\"data row1 col1\" >0.6429</td>\n",
              "      <td id=\"T_b5441_row1_col2\" class=\"data row1 col2\" >0.6290</td>\n",
              "      <td id=\"T_b5441_row1_col3\" class=\"data row1 col3\" >0.9512</td>\n",
              "      <td id=\"T_b5441_row1_col4\" class=\"data row1 col4\" >0.7573</td>\n",
              "      <td id=\"T_b5441_row1_col5\" class=\"data row1 col5\" >0.5673</td>\n",
              "      <td id=\"T_b5441_row1_col6\" class=\"data row1 col6\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b5441_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
              "      <td id=\"T_b5441_row2_col0\" class=\"data row2 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_b5441_row2_col1\" class=\"data row2 col1\" >0.7000</td>\n",
              "      <td id=\"T_b5441_row2_col2\" class=\"data row2 col2\" >0.7500</td>\n",
              "      <td id=\"T_b5441_row2_col3\" class=\"data row2 col3\" >0.7317</td>\n",
              "      <td id=\"T_b5441_row2_col4\" class=\"data row2 col4\" >0.7407</td>\n",
              "      <td id=\"T_b5441_row2_col5\" class=\"data row2 col5\" >0.7586</td>\n",
              "      <td id=\"T_b5441_row2_col6\" class=\"data row2 col6\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b5441_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_b5441_row3_col0\" class=\"data row3 col0\" >SVM (RBF, Tuned)</td>\n",
              "      <td id=\"T_b5441_row3_col1\" class=\"data row3 col1\" >0.5714</td>\n",
              "      <td id=\"T_b5441_row3_col2\" class=\"data row3 col2\" >0.6038</td>\n",
              "      <td id=\"T_b5441_row3_col3\" class=\"data row3 col3\" >0.7805</td>\n",
              "      <td id=\"T_b5441_row3_col4\" class=\"data row3 col4\" >0.6809</td>\n",
              "      <td id=\"T_b5441_row3_col5\" class=\"data row3 col5\" >0.6417</td>\n",
              "      <td id=\"T_b5441_row3_col6\" class=\"data row3 col6\" >{'clf__C': 10, 'clf__gamma': 0.01}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b5441_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
              "      <td id=\"T_b5441_row4_col0\" class=\"data row4 col0\" >SVC (Baseline)</td>\n",
              "      <td id=\"T_b5441_row4_col1\" class=\"data row4 col1\" >0.5571</td>\n",
              "      <td id=\"T_b5441_row4_col2\" class=\"data row4 col2\" >0.5962</td>\n",
              "      <td id=\"T_b5441_row4_col3\" class=\"data row4 col3\" >0.7561</td>\n",
              "      <td id=\"T_b5441_row4_col4\" class=\"data row4 col4\" >0.6667</td>\n",
              "      <td id=\"T_b5441_row4_col5\" class=\"data row4 col5\" >0.6165</td>\n",
              "      <td id=\"T_b5441_row4_col6\" class=\"data row4 col6\" >{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfFcL5zdTRV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Affect-3 Class\n"
      ],
      "metadata": {
        "id": "cKviJsORna28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Paths (Drive + runtime fallback)\n",
        "# -----------------------------\n",
        "def pick_path(*candidates):\n",
        "    for p in candidates:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of these paths exist:\\n\" + \"\\n\".join(candidates))\n",
        "\n",
        "OPENSMILE_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\",\n",
        "    \"/mnt/data/extracted_opensmile_features.csv\",\n",
        ")\n",
        "TRAIN_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\",\n",
        "    \"/mnt/data/stressidtrainbalanced2.csv\",\n",
        ")\n",
        "TEST_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\",\n",
        "    \"/mnt/data/stressidtest2.csv\",\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Load + merge on path\n",
        "# -----------------------------\n",
        "def norm_path(series: pd.Series) -> pd.Series:\n",
        "    return (series.astype(str).str.replace(\"\\\\\", \"/\", regex=False).str.strip())\n",
        "\n",
        "opensmile = pd.read_csv(OPENSMILE_PATH)\n",
        "train_df  = pd.read_csv(TRAIN_SPLIT_PATH)\n",
        "test_df   = pd.read_csv(TEST_SPLIT_PATH)\n",
        "\n",
        "opensmile[\"path_key\"] = norm_path(opensmile[\"file\"])\n",
        "train_df[\"path_key\"]  = norm_path(train_df[\"path\"])\n",
        "test_df[\"path_key\"]   = norm_path(test_df[\"path\"])\n",
        "\n",
        "opensmile_feats = opensmile.drop(columns=[\"file\"])\n",
        "train_merged = train_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "test_merged  = test_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "\n",
        "# -----------------------------\n",
        "# X/y for affect-3\n",
        "# -----------------------------\n",
        "LABEL_COL = \"affect3-class\"\n",
        "\n",
        "exclude_cols = {\n",
        "    \"subject/task\",\"binary-stress\",\"affect3-class\",\"subject\",\"isuseful\",\"path\",\n",
        "    \"continuouslabel\",\"basename\",\"path_key\"\n",
        "}\n",
        "feature_cols = [c for c in train_merged.columns if c not in exclude_cols]\n",
        "feature_cols = train_merged[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "X_train = train_merged[feature_cols]\n",
        "X_test  = test_merged[feature_cols]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_merged[LABEL_COL].astype(str))\n",
        "y_test  = le.transform(test_merged[LABEL_COL].astype(str))\n",
        "\n",
        "print(\"Classes (LabelEncoder):\", list(le.classes_))\n",
        "print(\"Test label counts:\", pd.Series(y_test).value_counts().sort_index().to_dict())\n",
        "\n",
        "# ---- class name mapping for your table (adjust if your dataset defines them differently)\n",
        "# Assumption: 0=relaxed, 1=neutral, 2=stressed\n",
        "class_name = {0: \"relaxed\", 1: \"neutral\", 2: \"stressed\"}\n",
        "\n",
        "# -----------------------------\n",
        "# CV setup (speaker-independent)\n",
        "# -----------------------------\n",
        "groups_train = train_merged[\"subject\"].astype(str) if \"subject\" in train_merged.columns else train_merged[\"path_key\"]\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def macro_auc_ovr_from_proba(y_true, proba):\n",
        "    return roc_auc_score(y_true, proba, multi_class=\"ovr\", average=\"macro\")\n",
        "\n",
        "def eval_multiclass(name, pipeline_str, repr_str, fitted_model, y_pred, y_proba_or_none):\n",
        "    bal = balanced_accuracy_score(y_test, y_pred)\n",
        "    f1m = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1s = f1_score(y_test, y_pred, average=None, labels=[0,1,2], zero_division=0)\n",
        "    auc = np.nan\n",
        "    if y_proba_or_none is not None:\n",
        "        auc = macro_auc_ovr_from_proba(y_test, y_proba_or_none)\n",
        "\n",
        "    row = {\n",
        "        \"Pipeline\": pipeline_str,\n",
        "        \"Representation\": repr_str,\n",
        "        \"Model\": name,\n",
        "        \"Bal Acc\": bal,\n",
        "        \"F1 (macro)\": f1m,\n",
        "        \"AUC (ROC-AUC)\": auc,\n",
        "        f\"F1 ({class_name[0]})\": f1s[0],\n",
        "        f\"F1 ({class_name[1]})\": f1s[1],\n",
        "        f\"F1 ({class_name[2]})\": f1s[2],\n",
        "    }\n",
        "    return row\n",
        "\n",
        "# -----------------------------\n",
        "# Baseline: always predict majority class (on TEST)\n",
        "# -----------------------------\n",
        "maj = int(pd.Series(y_test).value_counts().idxmax())\n",
        "y_pred_base = np.full_like(y_test, maj)\n",
        "baseline_row = eval_multiclass(\n",
        "    name=f\"Always predict class {maj}\",\n",
        "    pipeline_str=\"Baseline (Always predict majority class)\",\n",
        "    repr_str=\"-\",\n",
        "    fitted_model=None,\n",
        "    y_pred=y_pred_base,\n",
        "    y_proba_or_none=None\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Fit + evaluate the SAME model families as your original pipeline\n",
        "# -----------------------------\n",
        "rows = [baseline_row]\n",
        "\n",
        "PIPELINE_NAME = \"openSMILE (eGeMAPS) + best classical ML\"\n",
        "REPR_NAME     = \"eGeMAPSv02 (88d)\"  # keep as your table label (even though you used 37 prosodic dims)\n",
        "\n",
        "# 1) SVC (Baseline)  --- use probability=True so we can compute multiclass AUC\n",
        "svc_base = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42)),\n",
        "])\n",
        "svc_base.fit(X_train, y_train)\n",
        "rows.append(eval_multiclass(\n",
        "    \"SVC (Baseline)\", PIPELINE_NAME, REPR_NAME,\n",
        "    svc_base,\n",
        "    svc_base.predict(X_test),\n",
        "    svc_base.predict_proba(X_test),\n",
        "))\n",
        "\n",
        "# 2) Logistic Regression\n",
        "lr = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"lbfgs\", random_state=42)),\n",
        "])\n",
        "lr.fit(X_train, y_train)\n",
        "rows.append(eval_multiclass(\n",
        "    \"Logistic Reg.\", PIPELINE_NAME, REPR_NAME,\n",
        "    lr,\n",
        "    lr.predict(X_test),\n",
        "    lr.predict_proba(X_test),\n",
        "))\n",
        "\n",
        "# 3) Random Forest\n",
        "rf = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)),\n",
        "])\n",
        "rf.fit(X_train, y_train)\n",
        "rows.append(eval_multiclass(\n",
        "    \"Random Forest\", PIPELINE_NAME, REPR_NAME,\n",
        "    rf,\n",
        "    rf.predict(X_test),\n",
        "    rf.predict_proba(X_test),\n",
        "))\n",
        "\n",
        "# 4) Gradient Boosting (Tuned)\n",
        "gb_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", GradientBoostingClassifier(random_state=42)),\n",
        "])\n",
        "gb_grid = {\n",
        "    \"clf__n_estimators\": [100, 200],\n",
        "    \"clf__learning_rate\": [0.05, 0.1],\n",
        "    \"clf__max_depth\": [2, 3],\n",
        "    \"clf__subsample\": [1.0, 0.8],\n",
        "}\n",
        "gb_gs = GridSearchCV(gb_pipe, gb_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n",
        "gb_gs.fit(X_train, y_train, groups=groups_train)\n",
        "gb_best = gb_gs.best_estimator_\n",
        "rows.append(eval_multiclass(\n",
        "    \"GB (Tuned)\", PIPELINE_NAME, REPR_NAME,\n",
        "    gb_best,\n",
        "    gb_best.predict(X_test),\n",
        "    gb_best.predict_proba(X_test),\n",
        "))\n",
        "\n",
        "# 5) SVM (RBF, tuned)\n",
        "# Tune with probability=False (faster), then refit same best params with probability=True for AUC.\n",
        "svm_tune_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", probability=False, random_state=42)),\n",
        "])\n",
        "svm_grid = {\n",
        "    \"clf__C\": [0.1, 1, 10],\n",
        "    \"clf__gamma\": [\"scale\", 0.01, 0.1],\n",
        "}\n",
        "svm_gs = GridSearchCV(svm_tune_pipe, svm_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n",
        "svm_gs.fit(X_train, y_train, groups=groups_train)\n",
        "bestC = svm_gs.best_params_[\"clf__C\"]\n",
        "bestG = svm_gs.best_params_[\"clf__gamma\"]\n",
        "\n",
        "svm_best = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", C=bestC, gamma=bestG, probability=True, random_state=42)),\n",
        "])\n",
        "svm_best.fit(X_train, y_train)\n",
        "rows.append(eval_multiclass(\n",
        "    \"SVM (RBF, tuned)\", PIPELINE_NAME, REPR_NAME,\n",
        "    svm_best,\n",
        "    svm_best.predict(X_test),\n",
        "    svm_best.predict_proba(X_test),\n",
        "))\n",
        "\n",
        "# -----------------------------\n",
        "# Final table\n",
        "# -----------------------------\n",
        "out = pd.DataFrame(rows)\n",
        "\n",
        "# Order columns to match your screenshot\n",
        "ordered_cols = [\n",
        "    \"Pipeline\",\"Representation\",\"Model\",\n",
        "    \"Bal Acc\",\"F1 (macro)\",\"AUC (ROC-AUC)\",\n",
        "    f\"F1 ({class_name[0]})\", f\"F1 ({class_name[1]})\", f\"F1 ({class_name[2]})\"\n",
        "]\n",
        "out = out[ordered_cols]\n",
        "\n",
        "display(out.style.format({\n",
        "    \"Bal Acc\": \"{:.4f}\",\n",
        "    \"F1 (macro)\": \"{:.4f}\",\n",
        "    \"AUC (ROC-AUC)\": \"{:.4f}\",\n",
        "    f\"F1 ({class_name[0]})\": \"{:.4f}\",\n",
        "    f\"F1 ({class_name[1]})\": \"{:.4f}\",\n",
        "    f\"F1 ({class_name[2]})\": \"{:.4f}\",\n",
        "}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "h3Fx10CvP6vG",
        "outputId": "515de62c-183f-44ab-e06b-3e81de3b5e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes (LabelEncoder): ['0', '1', '2']\n",
            "Test label counts: {0: 11, 1: 32, 2: 27}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28da37b050>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_852b3\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_852b3_level0_col0\" class=\"col_heading level0 col0\" >Pipeline</th>\n",
              "      <th id=\"T_852b3_level0_col1\" class=\"col_heading level0 col1\" >Representation</th>\n",
              "      <th id=\"T_852b3_level0_col2\" class=\"col_heading level0 col2\" >Model</th>\n",
              "      <th id=\"T_852b3_level0_col3\" class=\"col_heading level0 col3\" >Bal Acc</th>\n",
              "      <th id=\"T_852b3_level0_col4\" class=\"col_heading level0 col4\" >F1 (macro)</th>\n",
              "      <th id=\"T_852b3_level0_col5\" class=\"col_heading level0 col5\" >AUC (ROC-AUC)</th>\n",
              "      <th id=\"T_852b3_level0_col6\" class=\"col_heading level0 col6\" >F1 (relaxed)</th>\n",
              "      <th id=\"T_852b3_level0_col7\" class=\"col_heading level0 col7\" >F1 (neutral)</th>\n",
              "      <th id=\"T_852b3_level0_col8\" class=\"col_heading level0 col8\" >F1 (stressed)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_852b3_row0_col0\" class=\"data row0 col0\" >Baseline (Always predict majority class)</td>\n",
              "      <td id=\"T_852b3_row0_col1\" class=\"data row0 col1\" >-</td>\n",
              "      <td id=\"T_852b3_row0_col2\" class=\"data row0 col2\" >Always predict class 1</td>\n",
              "      <td id=\"T_852b3_row0_col3\" class=\"data row0 col3\" >0.3333</td>\n",
              "      <td id=\"T_852b3_row0_col4\" class=\"data row0 col4\" >0.2092</td>\n",
              "      <td id=\"T_852b3_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
              "      <td id=\"T_852b3_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
              "      <td id=\"T_852b3_row0_col7\" class=\"data row0 col7\" >0.6275</td>\n",
              "      <td id=\"T_852b3_row0_col8\" class=\"data row0 col8\" >0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_852b3_row1_col0\" class=\"data row1 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_852b3_row1_col1\" class=\"data row1 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_852b3_row1_col2\" class=\"data row1 col2\" >SVC (Baseline)</td>\n",
              "      <td id=\"T_852b3_row1_col3\" class=\"data row1 col3\" >0.4058</td>\n",
              "      <td id=\"T_852b3_row1_col4\" class=\"data row1 col4\" >0.3776</td>\n",
              "      <td id=\"T_852b3_row1_col5\" class=\"data row1 col5\" >0.5928</td>\n",
              "      <td id=\"T_852b3_row1_col6\" class=\"data row1 col6\" >0.3448</td>\n",
              "      <td id=\"T_852b3_row1_col7\" class=\"data row1 col7\" >0.3396</td>\n",
              "      <td id=\"T_852b3_row1_col8\" class=\"data row1 col8\" >0.4483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_852b3_row2_col0\" class=\"data row2 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_852b3_row2_col1\" class=\"data row2 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_852b3_row2_col2\" class=\"data row2 col2\" >Logistic Reg.</td>\n",
              "      <td id=\"T_852b3_row2_col3\" class=\"data row2 col3\" >0.4939</td>\n",
              "      <td id=\"T_852b3_row2_col4\" class=\"data row2 col4\" >0.4588</td>\n",
              "      <td id=\"T_852b3_row2_col5\" class=\"data row2 col5\" >0.6260</td>\n",
              "      <td id=\"T_852b3_row2_col6\" class=\"data row2 col6\" >0.3871</td>\n",
              "      <td id=\"T_852b3_row2_col7\" class=\"data row2 col7\" >0.4074</td>\n",
              "      <td id=\"T_852b3_row2_col8\" class=\"data row2 col8\" >0.5818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_852b3_row3_col0\" class=\"data row3 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_852b3_row3_col1\" class=\"data row3 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_852b3_row3_col2\" class=\"data row3 col2\" >Random Forest</td>\n",
              "      <td id=\"T_852b3_row3_col3\" class=\"data row3 col3\" >0.5472</td>\n",
              "      <td id=\"T_852b3_row3_col4\" class=\"data row3 col4\" >0.5368</td>\n",
              "      <td id=\"T_852b3_row3_col5\" class=\"data row3 col5\" >0.5823</td>\n",
              "      <td id=\"T_852b3_row3_col6\" class=\"data row3 col6\" >0.6316</td>\n",
              "      <td id=\"T_852b3_row3_col7\" class=\"data row3 col7\" >0.4000</td>\n",
              "      <td id=\"T_852b3_row3_col8\" class=\"data row3 col8\" >0.5789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_852b3_row4_col0\" class=\"data row4 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_852b3_row4_col1\" class=\"data row4 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_852b3_row4_col2\" class=\"data row4 col2\" >GB (Tuned)</td>\n",
              "      <td id=\"T_852b3_row4_col3\" class=\"data row4 col3\" >0.4960</td>\n",
              "      <td id=\"T_852b3_row4_col4\" class=\"data row4 col4\" >0.4878</td>\n",
              "      <td id=\"T_852b3_row4_col5\" class=\"data row4 col5\" >0.6121</td>\n",
              "      <td id=\"T_852b3_row4_col6\" class=\"data row4 col6\" >0.5882</td>\n",
              "      <td id=\"T_852b3_row4_col7\" class=\"data row4 col7\" >0.3182</td>\n",
              "      <td id=\"T_852b3_row4_col8\" class=\"data row4 col8\" >0.5570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_852b3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_852b3_row5_col0\" class=\"data row5 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_852b3_row5_col1\" class=\"data row5 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_852b3_row5_col2\" class=\"data row5 col2\" >SVM (RBF, tuned)</td>\n",
              "      <td id=\"T_852b3_row5_col3\" class=\"data row5 col3\" >0.4362</td>\n",
              "      <td id=\"T_852b3_row5_col4\" class=\"data row5 col4\" >0.4015</td>\n",
              "      <td id=\"T_852b3_row5_col5\" class=\"data row5 col5\" >0.5816</td>\n",
              "      <td id=\"T_852b3_row5_col6\" class=\"data row5 col6\" >0.4348</td>\n",
              "      <td id=\"T_852b3_row5_col7\" class=\"data row5 col7\" >0.2553</td>\n",
              "      <td id=\"T_852b3_row5_col8\" class=\"data row5 col8\" >0.5143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === StressID/RAVDESS affect-3 classification using pre-extracted openSMILE features ===\n",
        "# Models (same as binary pipeline): SVM (RBF, tuned), GB (tuned), SVC (baseline), Logistic Regression, Random Forest\n",
        "# Assumes Google Drive is mounted.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, roc_auc_score\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Paths (Drive + fallback to uploaded runtime files)\n",
        "# -----------------------------\n",
        "def pick_path(*candidates):\n",
        "    for p in candidates:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of the provided paths exist:\\n\" + \"\\n\".join(candidates))\n",
        "\n",
        "OPENSMILE_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\",\n",
        "    \"/mnt/data/extracted_opensmile_features.csv\",\n",
        ")\n",
        "TRAIN_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\",\n",
        "    \"/mnt/data/stressidtrainbalanced2.csv\",\n",
        ")\n",
        "TEST_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\",\n",
        "    \"/mnt/data/stressidtest2.csv\",\n",
        ")\n",
        "\n",
        "print(\"Using:\")\n",
        "print(\" openSMILE:\", OPENSMILE_PATH)\n",
        "print(\" train:\", TRAIN_SPLIT_PATH)\n",
        "print(\" test :\", TEST_SPLIT_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load + merge on path\n",
        "# -----------------------------\n",
        "opensmile = pd.read_csv(OPENSMILE_PATH)\n",
        "train_df = pd.read_csv(TRAIN_SPLIT_PATH)\n",
        "test_df  = pd.read_csv(TEST_SPLIT_PATH)\n",
        "\n",
        "def norm_path(series: pd.Series) -> pd.Series:\n",
        "    return (series.astype(str)\n",
        "            .str.replace(\"\\\\\", \"/\", regex=False)\n",
        "            .str.strip())\n",
        "\n",
        "# Required columns (based on your earlier setup)\n",
        "# openSMILE: \"file\" ; split files: \"path\"\n",
        "opensmile[\"path_key\"] = norm_path(opensmile[\"file\"])\n",
        "train_df[\"path_key\"]  = norm_path(train_df[\"path\"])\n",
        "test_df[\"path_key\"]   = norm_path(test_df[\"path\"])\n",
        "\n",
        "opensmile_feats = opensmile.drop(columns=[\"file\"])\n",
        "train_merged = train_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "test_merged  = test_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "\n",
        "print(f\"Rows with any missing values after merge -> train: {train_merged.isna().any(axis=1).sum()}, \"\n",
        "      f\"test: {test_merged.isna().any(axis=1).sum()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Build X/y for affect-3 class\n",
        "# -----------------------------\n",
        "LABEL_COL = \"affect3-class\"  # column just to the right of binary-stress in your sheet\n",
        "\n",
        "exclude_cols = {\n",
        "    \"subject/task\", \"binary-stress\", \"affect3-class\", \"subject\", \"isuseful\", \"path\",\n",
        "    \"continuouslabel\", \"basename\", \"path_key\"\n",
        "}\n",
        "feature_cols = [c for c in train_merged.columns if c not in exclude_cols]\n",
        "feature_cols = train_merged[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No numeric feature columns found after merge. Check path matching and columns.\")\n",
        "\n",
        "X_train = train_merged[feature_cols]\n",
        "X_test  = test_merged[feature_cols]\n",
        "\n",
        "# Encode labels (works whether they're 0/1/2 or strings)\n",
        "le = LabelEncoder()\n",
        "y_train_raw = train_merged[LABEL_COL].astype(str)\n",
        "y_test_raw  = test_merged[LABEL_COL].astype(str)\n",
        "\n",
        "y_train = le.fit_transform(y_train_raw)\n",
        "y_test  = le.transform(y_test_raw)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
        "print(\"Classes:\", list(le.classes_))\n",
        "print(\"Train label distribution:\", pd.Series(y_train).value_counts().sort_index().to_dict())\n",
        "print(\"Test  label distribution:\", pd.Series(y_test).value_counts().sort_index().to_dict())\n",
        "\n",
        "# Group key for CV tuning (speaker-independent): prefer subject if present, else fallback\n",
        "groups_train = train_merged[\"subject\"].astype(str) if \"subject\" in train_merged.columns else train_merged[\"path_key\"]\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Define models (same set as before)\n",
        "# -----------------------------\n",
        "models = {}\n",
        "\n",
        "# SVC baseline (RBF default)\n",
        "models[\"SVC (Baseline)\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", random_state=42)),\n",
        "])\n",
        "\n",
        "# Logistic Regression (multiclass)\n",
        "models[\"Logistic Regression\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=4000,\n",
        "        solver=\"lbfgs\",\n",
        "        multi_class=\"auto\",\n",
        "        random_state=42\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Random Forest\n",
        "models[\"Random Forest\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Tuned SVM (RBF)\n",
        "svm_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", random_state=42)),\n",
        "])\n",
        "svm_grid = {\n",
        "    \"clf__C\": [0.1, 1, 10],\n",
        "    \"clf__gamma\": [\"scale\", 0.01, 0.1],\n",
        "}\n",
        "models[\"SVM (RBF, Tuned)\"] = GridSearchCV(\n",
        "    estimator=svm_pipe,\n",
        "    param_grid=svm_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "# Tuned Gradient Boosting (multiclass supported)\n",
        "gb_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", GradientBoostingClassifier(random_state=42)),\n",
        "])\n",
        "gb_grid = {\n",
        "    \"clf__n_estimators\": [100, 200],\n",
        "    \"clf__learning_rate\": [0.05, 0.1],\n",
        "    \"clf__max_depth\": [2, 3],\n",
        "    \"clf__subsample\": [1.0, 0.8],\n",
        "}\n",
        "models[\"Gradient Boosting (Tuned)\"] = GridSearchCV(\n",
        "    estimator=gb_pipe,\n",
        "    param_grid=gb_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Train + evaluate (multiclass metrics)\n",
        "# -----------------------------\n",
        "def try_multiclass_auc(fitted, X, y_true):\n",
        "    # Returns macro AUC (OVR) if possible, else NaN\n",
        "    try:\n",
        "        if hasattr(fitted, \"predict_proba\"):\n",
        "            scores = fitted.predict_proba(X)  # (n, K)\n",
        "            return roc_auc_score(y_true, scores, multi_class=\"ovr\", average=\"macro\")\n",
        "        if hasattr(fitted, \"decision_function\"):\n",
        "            scores = fitted.decision_function(X)  # (n, K) for multiclass SVC\n",
        "            return roc_auc_score(y_true, scores, multi_class=\"ovr\", average=\"macro\")\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "    return np.nan\n",
        "\n",
        "rows = []\n",
        "for name, model in models.items():\n",
        "    if isinstance(model, GridSearchCV):\n",
        "        model.fit(X_train, y_train, groups=groups_train)\n",
        "        best_params = model.best_params_\n",
        "        fitted = model.best_estimator_\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_params = {}\n",
        "        fitted = model\n",
        "\n",
        "    y_pred = fitted.predict(X_test)\n",
        "\n",
        "    rows.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Balanced Acc\": balanced_accuracy_score(y_test, y_pred),\n",
        "        \"Macro Precision\": precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"Macro Recall\": recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"Macro F1\": f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"Weighted F1\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"Macro ROC_AUC (OVR)\": try_multiclass_auc(fitted, X_test, y_test),\n",
        "        \"Best Params (if tuned)\": best_params,\n",
        "    })\n",
        "\n",
        "results = pd.DataFrame(rows).sort_values(\"Accuracy\", ascending=False)\n",
        "\n",
        "display(results.style.format({\n",
        "    \"Accuracy\": \"{:.4f}\",\n",
        "    \"Balanced Acc\": \"{:.4f}\",\n",
        "    \"Macro Precision\": \"{:.4f}\",\n",
        "    \"Macro Recall\": \"{:.4f}\",\n",
        "    \"Macro F1\": \"{:.4f}\",\n",
        "    \"Weighted F1\": \"{:.4f}\",\n",
        "    \"Macro ROC_AUC (OVR)\": \"{:.4f}\",\n",
        "}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "D1eDNv4GFpqN",
        "outputId": "ea711327-9a39-4186-d834-fdbe35451fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            " openSMILE: /content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\n",
            " train: /content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\n",
            " test : /content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\n",
            "Rows with any missing values after merge -> train: 0, test: 0\n",
            "Train shape: (440, 37)  Test shape: (70, 37)\n",
            "Classes: ['0', '1', '2']\n",
            "Train label distribution: {0: 149, 1: 133, 2: 158}\n",
            "Test  label distribution: {0: 11, 1: 32, 2: 27}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f28de65a390>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_964f2\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_964f2_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_964f2_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_964f2_level0_col2\" class=\"col_heading level0 col2\" >Balanced Acc</th>\n",
              "      <th id=\"T_964f2_level0_col3\" class=\"col_heading level0 col3\" >Macro Precision</th>\n",
              "      <th id=\"T_964f2_level0_col4\" class=\"col_heading level0 col4\" >Macro Recall</th>\n",
              "      <th id=\"T_964f2_level0_col5\" class=\"col_heading level0 col5\" >Macro F1</th>\n",
              "      <th id=\"T_964f2_level0_col6\" class=\"col_heading level0 col6\" >Weighted F1</th>\n",
              "      <th id=\"T_964f2_level0_col7\" class=\"col_heading level0 col7\" >Macro ROC_AUC (OVR)</th>\n",
              "      <th id=\"T_964f2_level0_col8\" class=\"col_heading level0 col8\" >Best Params (if tuned)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_964f2_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
              "      <td id=\"T_964f2_row0_col0\" class=\"data row0 col0\" >Random Forest</td>\n",
              "      <td id=\"T_964f2_row0_col1\" class=\"data row0 col1\" >0.5286</td>\n",
              "      <td id=\"T_964f2_row0_col2\" class=\"data row0 col2\" >0.5472</td>\n",
              "      <td id=\"T_964f2_row0_col3\" class=\"data row0 col3\" >0.6304</td>\n",
              "      <td id=\"T_964f2_row0_col4\" class=\"data row0 col4\" >0.5472</td>\n",
              "      <td id=\"T_964f2_row0_col5\" class=\"data row0 col5\" >0.5368</td>\n",
              "      <td id=\"T_964f2_row0_col6\" class=\"data row0 col6\" >0.5054</td>\n",
              "      <td id=\"T_964f2_row0_col7\" class=\"data row0 col7\" >0.5823</td>\n",
              "      <td id=\"T_964f2_row0_col8\" class=\"data row0 col8\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_964f2_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
              "      <td id=\"T_964f2_row1_col0\" class=\"data row1 col0\" >Gradient Boosting (Tuned)</td>\n",
              "      <td id=\"T_964f2_row1_col1\" class=\"data row1 col1\" >0.4857</td>\n",
              "      <td id=\"T_964f2_row1_col2\" class=\"data row1 col2\" >0.4960</td>\n",
              "      <td id=\"T_964f2_row1_col3\" class=\"data row1 col3\" >0.6132</td>\n",
              "      <td id=\"T_964f2_row1_col4\" class=\"data row1 col4\" >0.4960</td>\n",
              "      <td id=\"T_964f2_row1_col5\" class=\"data row1 col5\" >0.4878</td>\n",
              "      <td id=\"T_964f2_row1_col6\" class=\"data row1 col6\" >0.4527</td>\n",
              "      <td id=\"T_964f2_row1_col7\" class=\"data row1 col7\" >0.6121</td>\n",
              "      <td id=\"T_964f2_row1_col8\" class=\"data row1 col8\" >{'clf__learning_rate': 0.1, 'clf__max_depth': 2, 'clf__n_estimators': 200, 'clf__subsample': 0.8}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_964f2_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
              "      <td id=\"T_964f2_row2_col0\" class=\"data row2 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_964f2_row2_col1\" class=\"data row2 col1\" >0.4714</td>\n",
              "      <td id=\"T_964f2_row2_col2\" class=\"data row2 col2\" >0.4939</td>\n",
              "      <td id=\"T_964f2_row2_col3\" class=\"data row2 col3\" >0.4571</td>\n",
              "      <td id=\"T_964f2_row2_col4\" class=\"data row2 col4\" >0.4939</td>\n",
              "      <td id=\"T_964f2_row2_col5\" class=\"data row2 col5\" >0.4588</td>\n",
              "      <td id=\"T_964f2_row2_col6\" class=\"data row2 col6\" >0.4715</td>\n",
              "      <td id=\"T_964f2_row2_col7\" class=\"data row2 col7\" >0.6260</td>\n",
              "      <td id=\"T_964f2_row2_col8\" class=\"data row2 col8\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_964f2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_964f2_row3_col0\" class=\"data row3 col0\" >SVM (RBF, Tuned)</td>\n",
              "      <td id=\"T_964f2_row3_col1\" class=\"data row3 col1\" >0.4143</td>\n",
              "      <td id=\"T_964f2_row3_col2\" class=\"data row3 col2\" >0.4362</td>\n",
              "      <td id=\"T_964f2_row3_col3\" class=\"data row3 col3\" >0.4118</td>\n",
              "      <td id=\"T_964f2_row3_col4\" class=\"data row3 col4\" >0.4362</td>\n",
              "      <td id=\"T_964f2_row3_col5\" class=\"data row3 col5\" >0.4015</td>\n",
              "      <td id=\"T_964f2_row3_col6\" class=\"data row3 col6\" >0.3834</td>\n",
              "      <td id=\"T_964f2_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
              "      <td id=\"T_964f2_row3_col8\" class=\"data row3 col8\" >{'clf__C': 10, 'clf__gamma': 0.01}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_964f2_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
              "      <td id=\"T_964f2_row4_col0\" class=\"data row4 col0\" >SVC (Baseline)</td>\n",
              "      <td id=\"T_964f2_row4_col1\" class=\"data row4 col1\" >0.3857</td>\n",
              "      <td id=\"T_964f2_row4_col2\" class=\"data row4 col2\" >0.4058</td>\n",
              "      <td id=\"T_964f2_row4_col3\" class=\"data row4 col3\" >0.3752</td>\n",
              "      <td id=\"T_964f2_row4_col4\" class=\"data row4 col4\" >0.4058</td>\n",
              "      <td id=\"T_964f2_row4_col5\" class=\"data row4 col5\" >0.3776</td>\n",
              "      <td id=\"T_964f2_row4_col6\" class=\"data row4 col6\" >0.3823</td>\n",
              "      <td id=\"T_964f2_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
              "      <td id=\"T_964f2_row4_col8\" class=\"data row4 col8\" >{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ],
      "metadata": {
        "id": "j4lNGAJ0E06V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === StressID Regression (continuous label 010) using pre-extracted openSMILE features ===\n",
        "# Same \"family\" of models as before, but regression versions:\n",
        "# - SVM (RBF, tuned) -> SVR (RBF, tuned)\n",
        "# - GB (Tuned)       -> GradientBoostingRegressor (tuned)\n",
        "# - SVC (Baseline)   -> SVR (Baseline)\n",
        "# - Logistic Reg.    -> Ridge Regression (linear reg counterpart; logistic doesn't apply to regression)\n",
        "# - Random Forest    -> RandomForestRegressor\n",
        "#\n",
        "# Metrics: MAE, RMSE, % within 1 (|pred-true|<=1)\n",
        "# Assumes Drive is mounted.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Paths (Drive + fallback to uploaded runtime files)\n",
        "# -----------------------------\n",
        "def pick_path(*candidates):\n",
        "    for p in candidates:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of the provided paths exist:\\n\" + \"\\n\".join(candidates))\n",
        "\n",
        "OPENSMILE_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\",\n",
        "    \"/mnt/data/extracted_opensmile_features.csv\",\n",
        ")\n",
        "TRAIN_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\",\n",
        "    \"/mnt/data/stressidtrainbalanced2.csv\",\n",
        ")\n",
        "TEST_SPLIT_PATH = pick_path(\n",
        "    \"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\",\n",
        "    \"/mnt/data/stressidtest2.csv\",\n",
        ")\n",
        "\n",
        "print(\"Using:\")\n",
        "print(\" openSMILE:\", OPENSMILE_PATH)\n",
        "print(\" train:\", TRAIN_SPLIT_PATH)\n",
        "print(\" test :\", TEST_SPLIT_PATH)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load + merge on path\n",
        "# -----------------------------\n",
        "opensmile = pd.read_csv(OPENSMILE_PATH)\n",
        "train_df  = pd.read_csv(TRAIN_SPLIT_PATH)\n",
        "test_df   = pd.read_csv(TEST_SPLIT_PATH)\n",
        "\n",
        "def norm_path(series: pd.Series) -> pd.Series:\n",
        "    return (series.astype(str)\n",
        "            .str.replace(\"\\\\\", \"/\", regex=False)\n",
        "            .str.strip())\n",
        "\n",
        "opensmile[\"path_key\"] = norm_path(opensmile[\"file\"])\n",
        "train_df[\"path_key\"]  = norm_path(train_df[\"path\"])\n",
        "test_df[\"path_key\"]   = norm_path(test_df[\"path\"])\n",
        "\n",
        "opensmile_feats = opensmile.drop(columns=[\"file\"])\n",
        "train_merged = train_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "test_merged  = test_df.merge(opensmile_feats, on=\"path_key\", how=\"left\")\n",
        "\n",
        "print(f\"Rows with any missing values after merge -> train: {train_merged.isna().any(axis=1).sum()}, \"\n",
        "      f\"test: {test_merged.isna().any(axis=1).sum()}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Select continuous label (column G in split CSVs) robustly\n",
        "# -----------------------------\n",
        "def get_continuous_label(series_df: pd.DataFrame) -> pd.Series:\n",
        "    # Prefer a named column if present\n",
        "    for col in [\"continuouslabel\", \"continuous_label\", \"continuous label\", \"continuousLabel\", \"ContinuousLabel\"]:\n",
        "        if col in series_df.columns:\n",
        "            return pd.to_numeric(series_df[col], errors=\"coerce\")\n",
        "    # Fallback: \"column G\" = index 6 (0-based)\n",
        "    return pd.to_numeric(series_df.iloc[:, 6], errors=\"coerce\")\n",
        "\n",
        "y_train = get_continuous_label(train_merged).astype(float)\n",
        "y_test  = get_continuous_label(test_merged).astype(float)\n",
        "\n",
        "# Drop rows with missing targets just in case\n",
        "train_mask = y_train.notna()\n",
        "test_mask  = y_test.notna()\n",
        "\n",
        "train_merged = train_merged.loc[train_mask].reset_index(drop=True)\n",
        "test_merged  = test_merged.loc[test_mask].reset_index(drop=True)\n",
        "\n",
        "y_train = y_train.loc[train_mask].to_numpy()\n",
        "y_test  = y_test.loc[test_mask].to_numpy()\n",
        "\n",
        "# Features (same approach as before)\n",
        "exclude_cols = {\n",
        "    \"subject/task\", \"binary-stress\", \"affect3-class\", \"subject\", \"isuseful\", \"path\",\n",
        "    \"continuouslabel\", \"continuous_label\", \"continuous label\", \"continuousLabel\", \"ContinuousLabel\",\n",
        "    \"basename\", \"path_key\"\n",
        "}\n",
        "feature_cols = [c for c in train_merged.columns if c not in exclude_cols]\n",
        "feature_cols = train_merged[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "X_train = train_merged[feature_cols]\n",
        "X_test  = test_merged[feature_cols]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
        "print(\"y_train range:\", (np.nanmin(y_train), np.nanmax(y_train)), \" y_test range:\", (np.nanmin(y_test), np.nanmax(y_test)))\n",
        "\n",
        "# Groups for speaker-independent CV tuning (prefer subject if available)\n",
        "groups_train = train_merged[\"subject\"].astype(str) if \"subject\" in train_merged.columns else train_merged[\"path_key\"]\n",
        "cv = GroupKFold(n_splits=5)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Models (regression versions)\n",
        "# -----------------------------\n",
        "models = {}\n",
        "\n",
        "# SVR baseline\n",
        "models[\"SVR (Baseline)\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"reg\", SVR(kernel=\"rbf\")),\n",
        "])\n",
        "\n",
        "# \"Logistic Reg.\" counterpart for regression (linear baseline)\n",
        "models[\"Ridge Regression\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"reg\", Ridge(random_state=42)),\n",
        "])\n",
        "\n",
        "# Random Forest Regressor\n",
        "models[\"Random Forest\"] = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"reg\", RandomForestRegressor(\n",
        "        n_estimators=400,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Tuned SVR (RBF)\n",
        "svr_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"reg\", SVR(kernel=\"rbf\")),\n",
        "])\n",
        "svr_grid = {\n",
        "    \"reg__C\": [0.1, 1, 10],\n",
        "    \"reg__gamma\": [\"scale\", 0.01, 0.1],\n",
        "    \"reg__epsilon\": [0.1, 0.5, 1.0],\n",
        "}\n",
        "models[\"SVM (RBF, Tuned)\"] = GridSearchCV(\n",
        "    estimator=svr_pipe,\n",
        "    param_grid=svr_grid,\n",
        "    scoring=\"neg_mean_absolute_error\",  # optimize MAE\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Tuned Gradient Boosting Regressor\n",
        "gbr_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"reg\", GradientBoostingRegressor(random_state=42)),\n",
        "])\n",
        "gbr_grid = {\n",
        "    \"reg__n_estimators\": [100, 200],\n",
        "    \"reg__learning_rate\": [0.03, 0.1],\n",
        "    \"reg__max_depth\": [2, 3],\n",
        "    \"reg__subsample\": [1.0, 0.8],\n",
        "}\n",
        "models[\"GB (Tuned)\"] = GridSearchCV(\n",
        "    estimator=gbr_pipe,\n",
        "    param_grid=gbr_grid,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Evaluation helpers\n",
        "# -----------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def pct_within_1(y_true, y_pred):\n",
        "    return (np.abs(y_pred - y_true) <= 1.0).mean() * 100.0\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Baseline (always predict train mean)\n",
        "# -----------------------------\n",
        "train_mean = float(np.mean(y_train))\n",
        "y_pred_base = np.full_like(y_test, train_mean, dtype=float)\n",
        "\n",
        "rows = [{\n",
        "    \"Pipeline\": \"Baseline (Always predicts train mean)\",\n",
        "    \"Representation\": \"-\",\n",
        "    \"Model\": \"-\",\n",
        "    \"MAE\": mean_absolute_error(y_test, y_pred_base),\n",
        "    \"RMSE\": rmse(y_test, y_pred_base),\n",
        "    \"% within 1\": pct_within_1(y_test, y_pred_base),\n",
        "    \"Best Params (if tuned)\": {}\n",
        "}]\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Train + evaluate models\n",
        "# -----------------------------\n",
        "PIPELINE_NAME = \"openSMILE (eGeMAPS) + best classical ML\"\n",
        "REPR_NAME     = \"eGeMAPSv02 (88d)\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    if isinstance(model, GridSearchCV):\n",
        "        model.fit(X_train, y_train, groups=groups_train)\n",
        "        best_params = model.best_params_\n",
        "        fitted = model.best_estimator_\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_params = {}\n",
        "        fitted = model\n",
        "\n",
        "    y_pred = fitted.predict(X_test).astype(float)\n",
        "\n",
        "    rows.append({\n",
        "        \"Pipeline\": PIPELINE_NAME,\n",
        "        \"Representation\": REPR_NAME,\n",
        "        \"Model\": name,\n",
        "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
        "        \"RMSE\": rmse(y_test, y_pred),\n",
        "        \"% within 1\": pct_within_1(y_test, y_pred),\n",
        "        \"Best Params (if tuned)\": best_params\n",
        "    })\n",
        "\n",
        "results_reg = pd.DataFrame(rows).sort_values(\"MAE\", ascending=True)\n",
        "\n",
        "display(results_reg.style.format({\n",
        "    \"MAE\": \"{:.4f}\",\n",
        "    \"RMSE\": \"{:.4f}\",\n",
        "    \"% within 1\": \"{:.2f}\",\n",
        "}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "rxve-pcVFaiR",
        "outputId": "0a7fa519-d63d-4e60-87e7-12083c38b520"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            " openSMILE: /content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features.csv\n",
            " train: /content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\n",
            " test : /content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\n",
            "Rows with any missing values after merge -> train: 0, test: 0\n",
            "Train shape: (440, 37)  Test shape: (70, 37)\n",
            "y_train range: (np.float64(0.0), np.float64(10.0))  y_test range: (np.float64(1.0), np.float64(9.0))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7d63609adc70>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_4aa2b\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4aa2b_level0_col0\" class=\"col_heading level0 col0\" >Pipeline</th>\n",
              "      <th id=\"T_4aa2b_level0_col1\" class=\"col_heading level0 col1\" >Representation</th>\n",
              "      <th id=\"T_4aa2b_level0_col2\" class=\"col_heading level0 col2\" >Model</th>\n",
              "      <th id=\"T_4aa2b_level0_col3\" class=\"col_heading level0 col3\" >MAE</th>\n",
              "      <th id=\"T_4aa2b_level0_col4\" class=\"col_heading level0 col4\" >RMSE</th>\n",
              "      <th id=\"T_4aa2b_level0_col5\" class=\"col_heading level0 col5\" >% within 1</th>\n",
              "      <th id=\"T_4aa2b_level0_col6\" class=\"col_heading level0 col6\" >Best Params (if tuned)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
              "      <td id=\"T_4aa2b_row0_col0\" class=\"data row0 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_4aa2b_row0_col1\" class=\"data row0 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_4aa2b_row0_col2\" class=\"data row0 col2\" >Random Forest</td>\n",
              "      <td id=\"T_4aa2b_row0_col3\" class=\"data row0 col3\" >1.3586</td>\n",
              "      <td id=\"T_4aa2b_row0_col4\" class=\"data row0 col4\" >1.7339</td>\n",
              "      <td id=\"T_4aa2b_row0_col5\" class=\"data row0 col5\" >44.29</td>\n",
              "      <td id=\"T_4aa2b_row0_col6\" class=\"data row0 col6\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row1\" class=\"row_heading level0 row1\" >5</th>\n",
              "      <td id=\"T_4aa2b_row1_col0\" class=\"data row1 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_4aa2b_row1_col1\" class=\"data row1 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_4aa2b_row1_col2\" class=\"data row1 col2\" >GB (Tuned)</td>\n",
              "      <td id=\"T_4aa2b_row1_col3\" class=\"data row1 col3\" >1.3877</td>\n",
              "      <td id=\"T_4aa2b_row1_col4\" class=\"data row1 col4\" >1.7038</td>\n",
              "      <td id=\"T_4aa2b_row1_col5\" class=\"data row1 col5\" >42.86</td>\n",
              "      <td id=\"T_4aa2b_row1_col6\" class=\"data row1 col6\" >{'reg__learning_rate': 0.03, 'reg__max_depth': 2, 'reg__n_estimators': 100, 'reg__subsample': 0.8}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
              "      <td id=\"T_4aa2b_row2_col0\" class=\"data row2 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_4aa2b_row2_col1\" class=\"data row2 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_4aa2b_row2_col2\" class=\"data row2 col2\" >SVR (Baseline)</td>\n",
              "      <td id=\"T_4aa2b_row2_col3\" class=\"data row2 col3\" >1.5339</td>\n",
              "      <td id=\"T_4aa2b_row2_col4\" class=\"data row2 col4\" >1.8728</td>\n",
              "      <td id=\"T_4aa2b_row2_col5\" class=\"data row2 col5\" >38.57</td>\n",
              "      <td id=\"T_4aa2b_row2_col6\" class=\"data row2 col6\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
              "      <td id=\"T_4aa2b_row3_col0\" class=\"data row3 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_4aa2b_row3_col1\" class=\"data row3 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_4aa2b_row3_col2\" class=\"data row3 col2\" >SVM (RBF, Tuned)</td>\n",
              "      <td id=\"T_4aa2b_row3_col3\" class=\"data row3 col3\" >1.6706</td>\n",
              "      <td id=\"T_4aa2b_row3_col4\" class=\"data row3 col4\" >1.9216</td>\n",
              "      <td id=\"T_4aa2b_row3_col5\" class=\"data row3 col5\" >28.57</td>\n",
              "      <td id=\"T_4aa2b_row3_col6\" class=\"data row3 col6\" >{'reg__C': 0.1, 'reg__epsilon': 0.5, 'reg__gamma': 0.1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row4\" class=\"row_heading level0 row4\" >2</th>\n",
              "      <td id=\"T_4aa2b_row4_col0\" class=\"data row4 col0\" >openSMILE (eGeMAPS) + best classical ML</td>\n",
              "      <td id=\"T_4aa2b_row4_col1\" class=\"data row4 col1\" >eGeMAPSv02 (88d)</td>\n",
              "      <td id=\"T_4aa2b_row4_col2\" class=\"data row4 col2\" >Ridge Regression</td>\n",
              "      <td id=\"T_4aa2b_row4_col3\" class=\"data row4 col3\" >1.6804</td>\n",
              "      <td id=\"T_4aa2b_row4_col4\" class=\"data row4 col4\" >2.1008</td>\n",
              "      <td id=\"T_4aa2b_row4_col5\" class=\"data row4 col5\" >32.86</td>\n",
              "      <td id=\"T_4aa2b_row4_col6\" class=\"data row4 col6\" >{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4aa2b_level0_row5\" class=\"row_heading level0 row5\" >0</th>\n",
              "      <td id=\"T_4aa2b_row5_col0\" class=\"data row5 col0\" >Baseline (Always predicts train mean)</td>\n",
              "      <td id=\"T_4aa2b_row5_col1\" class=\"data row5 col1\" >-</td>\n",
              "      <td id=\"T_4aa2b_row5_col2\" class=\"data row5 col2\" >-</td>\n",
              "      <td id=\"T_4aa2b_row5_col3\" class=\"data row5 col3\" >1.7188</td>\n",
              "      <td id=\"T_4aa2b_row5_col4\" class=\"data row5 col4\" >1.9843</td>\n",
              "      <td id=\"T_4aa2b_row5_col5\" class=\"data row5 col5\" >28.57</td>\n",
              "      <td id=\"T_4aa2b_row5_col6\" class=\"data row5 col6\" >{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}