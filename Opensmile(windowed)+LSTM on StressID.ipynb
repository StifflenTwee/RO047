{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTcOCdbfe3hL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8974d00d-f3db-4e17-bbee-a9dbcf3b050c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch, torchaudio\n",
        "from google.colab import drive\n",
        "import os, pathlib, glob\n",
        "import opensmile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\".*TorchCodec.*\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\".*StreamReader.*deprecated.*\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequencing dataset\n"
      ],
      "metadata": {
        "id": "OLQvuWDE49WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SPLITCLIPS=\"/content/drive/MyDrive/WuHaoAllenCentad/stress_id_wav_filtered_split\"\n",
        "FEATURESCSV=\"/content/drive/MyDrive/WuHaoAllenCentad/extracted_opensmile_features_split.csv\"\n",
        "fdf=pd.read_csv(FEATURESCSV)\n",
        "stressidtestp=\"/content/drive/MyDrive/WuHaoAllenCentad/stressidtest2.csv\"\n",
        "stressidtest=pd.read_csv(stressidtestp)\n",
        "stressidtrainp=\"/content/drive/MyDrive/WuHaoAllenCentad/stressidtrainbalanced2.csv\"\n",
        "stressidtrain=pd.read_csv(stressidtrainp)\n",
        "\n",
        "id_and_categorical_cols = [\"file\", \"basename\"]\n",
        "opensmile_feature_cols = [col for col in fdf.columns\n",
        "                          if col not in id_and_categorical_cols]\n",
        "\n",
        "all_segments_data = []\n",
        "print(f\"Iterating through files in: {SPLITCLIPS}\")\n",
        "for filename in os.listdir(SPLITCLIPS):\n",
        "    filepath = os.path.join(SPLITCLIPS, filename)\n",
        "    if os.path.isfile(filepath):\n",
        "      filename_full = filename.split('/')[-1]\n",
        "      original_file_id = '_'.join(filename_full.split('_')[:2])\n",
        "      start_time_str = filename_full.split(\"_\")[2].strip(\"s\")\n",
        "      segment_index = int(start_time_str) // 10\n",
        "\n",
        "      features = fdf.loc[fdf['file'] == filepath].iloc[0]\n",
        "\n",
        "      all_segments_data.append({\n",
        "          \"original_file_id\": original_file_id,\n",
        "          \"segment_index\": segment_index,\n",
        "          \"features\": features,\n",
        "      })\n",
        "\n",
        "grouped_by_original_file = {}\n",
        "for segment in all_segments_data:\n",
        "    orig_id = segment[\"original_file_id\"]\n",
        "    if orig_id not in grouped_by_original_file:\n",
        "        grouped_by_original_file[orig_id] = []\n",
        "    grouped_by_original_file[orig_id].append(segment)\n",
        "\n",
        "combined_stress_df = pd.concat([stressidtrain, stressidtest])\n",
        "stress_labels_map = combined_stress_df.set_index('subject/task')['binary-stress'].to_dict()\n",
        "\n",
        "filtered_grouped_by_original_file = {}\n",
        "y_labels_for_sequences = []\n",
        "\n",
        "for orig_id, segments in grouped_by_original_file.items():\n",
        "    if orig_id in stress_labels_map:\n",
        "        filtered_grouped_by_original_file[orig_id] = segments\n",
        "\n",
        "grouped_by_original_file = filtered_grouped_by_original_file\n",
        "\n",
        "for orig_id in grouped_by_original_file:\n",
        "    grouped_by_original_file[orig_id].sort(key=lambda x: x[\"segment_index\"])\n",
        "\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "original_file_ids_list = []\n",
        "\n",
        "sorted_opensmile_features = sorted(opensmile_feature_cols)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for orig_id in grouped_by_original_file:\n",
        "    current_X = []\n",
        "    segments = grouped_by_original_file[orig_id]\n",
        "    for segment in segments:\n",
        "        feature_vector = [segment[\"features\"][col] for col in sorted_opensmile_features]\n",
        "        current_X.append(feature_vector)\n",
        "\n",
        "    if current_X:\n",
        "        X_sequences.append(np.array(current_X))\n",
        "        y_sequences.append(np.array([stress_labels_map[orig_id]], dtype=np.float32))\n",
        "        original_file_ids_list.append(orig_id)\n",
        "\n",
        "print(f\"Total number of sequences (original audio files): {len(X_sequences)}\")\n",
        "if X_sequences:\n",
        "    print(f\"Shape of X[0] sequence (features): {X_sequences[0].shape} (sequence_length, num_opensmile_features)\")\n",
        "    print(f\"Shape of y[0] sequence (label): {y_sequences[0].shape} (1,)\")\n",
        "\n",
        "newdf = {\n",
        "    \"X_sequences\": X_sequences,\n",
        "    \"y_sequences\": y_sequences,\n",
        "    \"original_file_ids\": original_file_ids_list,\n",
        "    \"feature_names\": sorted_opensmile_features,\n",
        "    \"label_names\": [\"binary_stress\"]\n",
        "}\n",
        "\n",
        "print(f\"# of original audio files processed: {len(newdf['X_sequences'])}\")\n",
        "if newdf['X_sequences']:\n",
        "    print(f\"# of features per segment: {len(newdf['feature_names'])}\")\n",
        "    print(f\"# of labels per sequence: {len(newdf['label_names'])}\")\n",
        "    print(f\"Shape of features for the first file (sequence): {newdf['X_sequences'][0].shape}\")\n",
        "    print(f\"Shape of label for the first file: {newdf['y_sequences'][0].shape}\")"
      ],
      "metadata": {
        "id": "uYuSqtij49Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split into train and test based on stressidtrainbalanced2.csv and stressidtest2.csv"
      ],
      "metadata": {
        "id": "HLGtvDQ3SJh-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9264ef45"
      },
      "source": [
        "train_ids = set(stressidtrain['subject/task'].values)\n",
        "test_ids = set(stressidtest['subject/task'].values)\n",
        "\n",
        "X_train, y_train, original_file_ids_train = [], [], []\n",
        "X_test, y_test, original_file_ids_test = [], [], []\n",
        "\n",
        "for i, file_id in enumerate(newdf['original_file_ids']):\n",
        "    if file_id in train_ids:\n",
        "        X_train.append(newdf['X_sequences'][i])\n",
        "        y_train.append(newdf['y_sequences'][i])\n",
        "        original_file_ids_train.append(file_id)\n",
        "    elif file_id in test_ids:\n",
        "        X_test.append(newdf['X_sequences'][i])\n",
        "        y_test.append(newdf['y_sequences'][i])\n",
        "        original_file_ids_test.append(file_id)\n",
        "    else:\n",
        "        print(f\"{file_id} not found, skipping.\")\n",
        "\n",
        "traindf = {\n",
        "    \"X_sequences\": X_train,\n",
        "    \"y_sequences\": y_train,\n",
        "    \"original_file_ids\": original_file_ids_train,\n",
        "    \"feature_names\": newdf['feature_names'],\n",
        "    \"label_names\": newdf['label_names']\n",
        "}\n",
        "\n",
        "testdf = {\n",
        "    \"X_sequences\": X_test,\n",
        "    \"y_sequences\": y_test,\n",
        "    \"original_file_ids\": original_file_ids_test,\n",
        "    \"feature_names\": newdf['feature_names'],\n",
        "    \"label_names\": newdf['label_names']\n",
        "}\n",
        "\n",
        "print(f\"Number of sequences in traindf: {len(traindf['X_sequences'])}\")\n",
        "print(f\"Number of sequences in testdf: {len(testdf['X_sequences'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset loader / batcher"
      ],
      "metadata": {
        "id": "U-n89yxeSiKV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3b8a79a",
        "outputId": "20069c9e-d97b-482c-fbbe-1141c16f88eb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "newdf_lookup = {}\n",
        "for i, file_id in enumerate(newdf['original_file_ids']):\n",
        "    newdf_lookup[file_id] = (\n",
        "        newdf['X_sequences'][i],\n",
        "        newdf['y_sequences'][i]\n",
        "    )\n",
        "\n",
        "X_train, y_train, original_file_ids_train = [], [], []\n",
        "X_test, y_test, original_file_ids_test = [], [], []\n",
        "\n",
        "for file_id in stressidtrain['subject/task']:\n",
        "    if file_id in newdf_lookup:\n",
        "        x_seq, y_seq = newdf_lookup[file_id]\n",
        "        X_train.append(x_seq)\n",
        "        y_train.append(y_seq)\n",
        "        original_file_ids_train.append(file_id)\n",
        "    else:\n",
        "        print(f\"Warning: Training file_id '{file_id}' not found in newdf_lookup. Skipping.\")\n",
        "\n",
        "for file_id in stressidtest['subject/task'].unique():\n",
        "    if file_id in newdf_lookup:\n",
        "        x_seq, y_seq = newdf_lookup[file_id]\n",
        "        X_test.append(x_seq)\n",
        "        y_test.append(y_seq)\n",
        "        original_file_ids_test.append(file_id)\n",
        "    else:\n",
        "        print(f\"Warning: Test file_id '{file_id}' not found in newdf_lookup. Skipping.\")\n",
        "\n",
        "traindf = {\n",
        "    \"X_sequences\": X_train,\n",
        "    \"y_sequences\": y_train,\n",
        "    \"original_file_ids\": original_file_ids_train,\n",
        "    \"feature_names\": newdf['feature_names'],\n",
        "    \"label_names\": newdf['label_names']\n",
        "}\n",
        "\n",
        "testdf = {\n",
        "    \"X_sequences\": X_test,\n",
        "    \"y_sequences\": y_test,\n",
        "    \"original_file_ids\": original_file_ids_test,\n",
        "    \"feature_names\": newdf['feature_names'],\n",
        "    \"label_names\": newdf['label_names']\n",
        "}\n",
        "\n",
        "print(f\"Number of sequences in traindf: {len(traindf['X_sequences'])}\")\n",
        "print(f\"Number of sequences in testdf: {len(testdf['X_sequences'])}\")\n",
        "\n",
        "unique_train_original_ids = set(stressidtrain['subject/task'].unique())\n",
        "unique_test_original_ids = set(stressidtest['subject/task'].unique())\n",
        "\n",
        "assert len(unique_train_original_ids.intersection(unique_test_original_ids)) == 0, \"Train and Test sets have overlapping original_file_ids!\"\n",
        "print(\"Train and Test sets have no overlapping original_file_ids based on unique subject/task identifiers.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences in traindf: 440\n",
            "Number of sequences in testdf: 70\n",
            "Train and Test sets have no overlapping original_file_ids based on unique subject/task identifiers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cb6616a",
        "outputId": "bbcf64f4-1cce-4837-afb8-d97bcb2785dc"
      },
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "TRAIN_DATA_FILE = \"/content/drive/MyDrive/WuHaoAllenCentad/traindf_sequenced_data.joblib\"\n",
        "TEST_DATA_FILE = \"/content/drive/MyDrive/WuHaoAllenCentad/testdf_sequenced_data.joblib\"\n",
        "\n",
        "os.makedirs(os.path.dirname(TRAIN_DATA_FILE), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(TEST_DATA_FILE), exist_ok=True)\n",
        "\n",
        "joblib.dump(traindf, TRAIN_DATA_FILE)\n",
        "joblib.dump(testdf, TEST_DATA_FILE)\n",
        "\n",
        "print(f\"Training data saved to: {TRAIN_DATA_FILE}\")\n",
        "print(f\"Testing data saved to: {TEST_DATA_FILE}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data saved to: /content/drive/MyDrive/WuHaoAllenCentad/traindf_sequenced_data.joblib\n",
            "Testing data saved to: /content/drive/MyDrive/WuHaoAllenCentad/testdf_sequenced_data.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ded88f",
        "outputId": "d492b06e-76b3-4d2b-ac8b-22951fb8a37f"
      },
      "source": [
        "import joblib\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "TRAIN_DATA_FILE = \"/content/drive/MyDrive/WuHaoAllenCentad/traindf_sequenced_data.joblib\"\n",
        "TEST_DATA_FILE = \"/content/drive/MyDrive/WuHaoAllenCentad/testdf_sequenced_data.joblib\"\n",
        "\n",
        "traindf = joblib.load(TRAIN_DATA_FILE)\n",
        "testdf = joblib.load(TEST_DATA_FILE)\n",
        "\n",
        "print(f\"Training data loaded from: {TRAIN_DATA_FILE}\")\n",
        "print(f\"Testing data loaded from: {TEST_DATA_FILE}\")\n",
        "\n",
        "class StressDataset(Dataset):\n",
        "    def __init__(self, X_sequences, y_sequences):\n",
        "        self.X_sequences = X_sequences\n",
        "        self.y_sequences = y_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_tensor = torch.from_numpy(self.X_sequences[idx]).float()\n",
        "        y_tensor = torch.from_numpy(self.y_sequences[idx]).float()\n",
        "        return x_tensor, y_tensor\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    x_batch, y_batch = zip(*batch)\n",
        "    padded_x = pad_sequence(x_batch, batch_first=True, padding_value=0.0)\n",
        "    stacked_y = torch.stack(y_batch, dim=0)\n",
        "    return padded_x, stacked_y\n",
        "\n",
        "train_dataset = StressDataset(traindf['X_sequences'], traindf['y_sequences'])\n",
        "test_dataset = StressDataset(testdf['X_sequences'], testdf['y_sequences'])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"\\nNumber of sequences in train_dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of sequences in test_dataset: {len(test_dataset)}\")\n",
        "print(f\"Train DataLoader created with {len(train_loader)} batches.\")\n",
        "print(f\"Test DataLoader created with {len(test_loader)} batches.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Training data loaded from: /content/drive/MyDrive/WuHaoAllenCentad/traindf_sequenced_data.joblib\n",
            "Testing data loaded from: /content/drive/MyDrive/WuHaoAllenCentad/testdf_sequenced_data.joblib\n",
            "\n",
            "Number of sequences in train_dataset: 440\n",
            "Number of sequences in test_dataset: 70\n",
            "Train DataLoader created with 14 batches.\n",
            "Test DataLoader created with 3 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "ZQICpAkbaFv2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f8466b5"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class StressLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, bidirectional=False, dropout_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_prob if num_layers > 1 else 0\n",
        "        )\n",
        "        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc = nn.Linear(self.output_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            last_hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            last_hidden = h_n[-1,:,:]\n",
        "\n",
        "        last_hidden = self.dropout(last_hidden)\n",
        "\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model"
      ],
      "metadata": {
        "id": "X5DEEisDaI00"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199f6d68",
        "outputId": "7c56d312-f211-44e0-c181-2f0c74c35425"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "input_dim = len(traindf['feature_names'])\n",
        "\n",
        "model = StressLSTM(input_dim=input_dim, hidden_dim=128, num_layers=3, bidirectional=False, dropout_prob=0.2)\n",
        "\n",
        "y_train_flat = np.concatenate(traindf['y_sequences'])\n",
        "num_pos = np.sum(y_train_flat == 1)\n",
        "num_neg = np.sum(y_train_flat == 0)\n",
        "\n",
        "print(f\"# stress: {num_pos}\")\n",
        "print(f\"# non stressed {num_neg}\")\n",
        "\n",
        "pos_weight_value = torch.tensor([num_neg / num_pos], dtype=torch.float32) if num_pos > 0 else torch.tensor([1.0])\n",
        "\n",
        "print(f\"Calculated positive class weight: {pos_weight_value.item():.4f}\")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 50\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(batch_X)\n",
        "\n",
        "        loss = criterion(preds, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(train_dataset):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# stress: 220\n",
            "# non stressed 220\n",
            "Calculated positive class weight: 1.0000\n",
            "Starting training for 50 epochs...\n",
            "Epoch 1/50, Loss: 0.6968\n",
            "Epoch 2/50, Loss: 0.6799\n",
            "Epoch 3/50, Loss: 0.6297\n",
            "Epoch 4/50, Loss: 0.5871\n",
            "Epoch 5/50, Loss: 0.5610\n",
            "Epoch 6/50, Loss: 0.4952\n",
            "Epoch 7/50, Loss: 0.4384\n",
            "Epoch 8/50, Loss: 0.4042\n",
            "Epoch 9/50, Loss: 0.3408\n",
            "Epoch 10/50, Loss: 0.3179\n",
            "Epoch 11/50, Loss: 0.4014\n",
            "Epoch 12/50, Loss: 0.2893\n",
            "Epoch 13/50, Loss: 0.2007\n",
            "Epoch 14/50, Loss: 0.2340\n",
            "Epoch 15/50, Loss: 0.1966\n",
            "Epoch 16/50, Loss: 0.1372\n",
            "Epoch 17/50, Loss: 0.1391\n",
            "Epoch 18/50, Loss: 0.2490\n",
            "Epoch 19/50, Loss: 0.1516\n",
            "Epoch 20/50, Loss: 0.1241\n",
            "Epoch 21/50, Loss: 0.1045\n",
            "Epoch 22/50, Loss: 0.1028\n",
            "Epoch 23/50, Loss: 0.1139\n",
            "Epoch 24/50, Loss: 0.1711\n",
            "Epoch 25/50, Loss: 0.1683\n",
            "Epoch 26/50, Loss: 0.1023\n",
            "Epoch 27/50, Loss: 0.0847\n",
            "Epoch 28/50, Loss: 0.0994\n",
            "Epoch 29/50, Loss: 0.1181\n",
            "Epoch 30/50, Loss: 0.0999\n",
            "Epoch 31/50, Loss: 0.0519\n",
            "Epoch 32/50, Loss: 0.0434\n",
            "Epoch 33/50, Loss: 0.0882\n",
            "Epoch 34/50, Loss: 0.0776\n",
            "Epoch 35/50, Loss: 0.0632\n",
            "Epoch 36/50, Loss: 0.0672\n",
            "Epoch 37/50, Loss: 0.0899\n",
            "Epoch 38/50, Loss: 0.0926\n",
            "Epoch 39/50, Loss: 0.1118\n",
            "Epoch 40/50, Loss: 0.1417\n",
            "Epoch 41/50, Loss: 0.0978\n",
            "Epoch 42/50, Loss: 0.0632\n",
            "Epoch 43/50, Loss: 0.0501\n",
            "Epoch 44/50, Loss: 0.0685\n",
            "Epoch 45/50, Loss: 0.0372\n",
            "Epoch 46/50, Loss: 0.0335\n",
            "Epoch 47/50, Loss: 0.0184\n",
            "Epoch 48/50, Loss: 0.0212\n",
            "Epoch 49/50, Loss: 0.0673\n",
            "Epoch 50/50, Loss: 0.0455\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c32855"
      },
      "source": [
        "## Evaluate LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2516adda",
        "outputId": "38fbb48e-2ce5-4898-9026-405ae9b97a36"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\"\n",
        "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Trained model saved to: {MODEL_SAVE_PATH}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved to: /content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b394cc2",
        "outputId": "62688a20-e75d-4236-f2ef-1e2c7f6fe6e6"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# MODEL_SAVE_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_model3.pth\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\"\n",
        "\n",
        "input_dim = len(traindf['feature_names'])\n",
        "model = StressLSTM(input_dim=input_dim, hidden_dim=128, num_layers=3, bidirectional=False, dropout_prob=0.2)\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "print(f\"model loaded from: {MODEL_SAVE_PATH}\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from: /content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0255ed81",
        "outputId": "88551ffb-e844-4a12-9a3c-4d55960880f9"
      },
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        logits = model(batch_X)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        all_probs.extend(probs.cpu().numpy().flatten())\n",
        "        all_labels.extend(batch_y.cpu().numpy().flatten())\n",
        "all_probs = np.array(all_probs)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "final_threshold = 0.5\n",
        "\n",
        "all_preds = (all_probs >= final_threshold).astype(int)\n",
        "\n",
        "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "auc_roc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "precision_class1 = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
        "recall_class1 = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
        "precision_class0 = precision_score(all_labels, all_preds, pos_label=0, zero_division=0)\n",
        "recall_class0 = recall_score(all_labels, all_preds, pos_label=0, zero_division=0)\n",
        "\n",
        "print(\"--- Comprehensive Evaluation Metrics ---\")\n",
        "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
        "print(f\"F1 (macro): {f1_macro:.4f}\")\n",
        "print(f\"AUC (ROC-AUC): {auc_roc:.4f}\")\n",
        "print(f\"\\nPrecision (Class 1 - Stressed): {precision_class1:.4f}\")\n",
        "print(f\"Recall (Class 1 - Stressed): {recall_class1:.4f}\")\n",
        "print(f\"Precision (Class 0 - Non-Stressed): {precision_class0:.4f}\")\n",
        "print(f\"Recall (Class 0 - Non-Stressed): {recall_class0:.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Comprehensive Evaluation Metrics ---\n",
            "Balanced Accuracy: 0.6560\n",
            "F1 (macro): 0.6578\n",
            "AUC (ROC-AUC): 0.7317\n",
            "\n",
            "Precision (Class 1 - Stressed): 0.6939\n",
            "Recall (Class 1 - Stressed): 0.8293\n",
            "Precision (Class 0 - Non-Stressed): 0.6667\n",
            "Recall (Class 0 - Non-Stressed): 0.4828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de9fd225"
      },
      "source": [
        "# Getting predicted probabilities for ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "54239cb6",
        "outputId": "1eda6b36-718a-4517-f29b-804c7836529a"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_model.pth\"\n",
        "\n",
        "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True);\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH);\n",
        "\n",
        "print(f\"Trained model saved to: {MODEL_SAVE_PATH}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1457478875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3. Save the trained model's state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 4. Print a confirmation message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e303a24",
        "outputId": "c6dea326-1a12-4e0a-ca57-4126212b230d"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\"\n",
        "\n",
        "input_dim = len(traindf['feature_names'])\n",
        "model = StressLSTM(input_dim=input_dim, hidden_dim=128, num_layers=3, bidirectional=False, dropout_prob=0.2)\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "print(f\"model loaded from: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "train_ids = []\n",
        "train_probs = []\n",
        "\n",
        "print(f\"Predicting probabilities for {len(traindf['X_sequences'])} training sequences...\")\n",
        "for i, X_sequence in enumerate(traindf['X_sequences']):\n",
        "    original_file_id = traindf['original_file_ids'][i]\n",
        "\n",
        "    feature_tensor = torch.from_numpy(X_sequence).float().unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(feature_tensor)\n",
        "\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    probability = probs.item()\n",
        "\n",
        "    train_ids.append(original_file_id)\n",
        "    train_probs.append(probability)\n",
        "\n",
        "print(\"Probability prediction for training data complete.\")\n",
        "print(f\"Collected {len(train_ids)} train IDs and {len(train_probs)} train probabilities.\")\n",
        "\n",
        "print(\"\\nFirst 5 predicted training probabilities:\")\n",
        "for i in range(min(5, len(train_ids))):\n",
        "    print(f\"ID: {train_ids[i]}, Probability: {train_probs[i]:.4f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from: /content/drive/MyDrive/WuHaoAllenCentad/stress_lstm_modellayer3.pth\n",
            "Predicting probabilities for 440 training sequences...\n",
            "Probability prediction for training data complete.\n",
            "Collected 440 train IDs and 440 train probabilities.\n",
            "\n",
            "First 5 predicted training probabilities:\n",
            "ID: tmvd_Stroop, Probability: 0.0048\n",
            "ID: 9t6n_Counting2, Probability: 0.9987\n",
            "ID: kycf_Counting3, Probability: 0.0039\n",
            "ID: t6v9_Math, Probability: 0.9993\n",
            "ID: y8c3_Speaking, Probability: 0.0603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775e1a42"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_ids = []\n",
        "test_probs = []\n",
        "\n",
        "for i, X_sequence in enumerate(testdf['X_sequences']):\n",
        "    original_file_id = testdf['original_file_ids'][i]\n",
        "\n",
        "    feature_tensor = torch.from_numpy(X_sequence).float().unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(feature_tensor)\n",
        "\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    probability = probs.item()\n",
        "\n",
        "    test_ids.append(original_file_id)\n",
        "    test_probs.append(probability)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab68d019",
        "outputId": "e3dc1e5a-1131-4dcb-907a-0c898ce39546"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "train_predictions_df = pd.DataFrame({\n",
        "    'subject/task': train_ids,\n",
        "    'stress_probability': train_probs,\n",
        "    'dataset': 'train'\n",
        "})\n",
        "\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'subject/task': test_ids,\n",
        "    'stress_probability': test_probs,\n",
        "    'dataset': 'test'\n",
        "})\n",
        "\n",
        "all_predictions_df = pd.concat([train_predictions_df, test_predictions_df], ignore_index=True);\n",
        "\n",
        "SAVE_CSV_PATH = \"/content/drive/MyDrive/WuHaoAllenCentad/audiosegmentationopensmilepredictions.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_CSV_PATH), exist_ok=True);\n",
        "\n",
        "all_predictions_df.to_csv(SAVE_CSV_PATH, index=False);\n",
        "\n",
        "print(f\"All stress predictions saved to: {SAVE_CSV_PATH}\")\n",
        "print(\"First 5 rows of the combined predictions DataFrame:\")\n",
        "print(all_predictions_df.head())\n",
        "print(\"Last 5 rows of the combined predictions DataFrame:\")\n",
        "print(all_predictions_df.tail())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All stress predictions saved to: /content/drive/MyDrive/WuHaoAllenCentad/audiosegmentationopensmilepredictions.csv\n",
            "First 5 rows of the combined predictions DataFrame:\n",
            "     subject/task  stress_probability dataset\n",
            "0     tmvd_Stroop            0.004798   train\n",
            "1  9t6n_Counting2            0.998699   train\n",
            "2  kycf_Counting3            0.003889   train\n",
            "3       t6v9_Math            0.999322   train\n",
            "4   y8c3_Speaking            0.060315   train\n",
            "Last 5 rows of the combined predictions DataFrame:\n",
            "       subject/task  stress_probability dataset\n",
            "505  y9z6_Counting3            0.997287    test\n",
            "506       y9z6_Math            0.996912    test\n",
            "507    y9z6_Reading            0.996053    test\n",
            "508   y9z6_Speaking            0.999095    test\n",
            "509     y9z6_Stroop            0.992305    test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One item prediction"
      ],
      "metadata": {
        "id": "Hl_I0F7xZEGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "def analyze_stress_segments(main_mp3_path, chunk_duration_sec=30):\n",
        "    if not os.path.exists(main_mp3_path):\n",
        "        print(f\"File not found: {main_mp3_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading {main_mp3_path}...\")\n",
        "    audio = AudioSegment.from_mp3(main_mp3_path)\n",
        "    total_duration_sec = len(audio) / 1000\n",
        "    print(f\"Total duration: {total_duration_sec:.2f} seconds\")\n",
        "\n",
        "    results = []\n",
        "    chunk_len_ms = chunk_duration_sec * 1000\n",
        "\n",
        "    chunk_temp_dir = \"temp_30s_chunks\"\n",
        "    if os.path.exists(chunk_temp_dir):\n",
        "        shutil.rmtree(chunk_temp_dir)\n",
        "    os.makedirs(chunk_temp_dir)\n",
        "\n",
        "    try:\n",
        "        for i, start_ms in enumerate(range(0, len(audio), chunk_len_ms)):\n",
        "            end_ms = min(start_ms + chunk_len_ms, len(audio))\n",
        "\n",
        "            if end_ms - start_ms < 10000:\n",
        "                print(f\"Skipping tail segment {start_ms/1000:.1f}s-{end_ms/1000:.1f}s (too short)\")\n",
        "                continue\n",
        "\n",
        "            chunk = audio[start_ms:end_ms]\n",
        "\n",
        "            chunk_filename = f\"chunk_{i}_{start_ms//1000}-{end_ms//1000}.mp3\"\n",
        "            chunk_path = os.path.join(chunk_temp_dir, chunk_filename)\n",
        "            chunk.export(chunk_path, format=\"mp3\")\n",
        "\n",
        "            print(f\"Analyzing Window {i+1}: {start_ms/1000:.1f}s - {end_ms/1000:.1f}s\")\n",
        "\n",
        "            result = predict_stress_sliding_window(chunk_path)\n",
        "\n",
        "            if result is not None:\n",
        "                pred, prob = result\n",
        "                results.append({\n",
        "                    \"window_index\": i + 1,\n",
        "                    \"start_time_s\": start_ms / 1000,\n",
        "                    \"end_time_s\": end_ms / 1000,\n",
        "                    \"predicted_class\": pred,\n",
        "                    \"stress_probability\": prob\n",
        "                })\n",
        "\n",
        "            os.remove(chunk_path)\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(chunk_temp_dir):\n",
        "            shutil.rmtree(chunk_temp_dir)\n",
        "\n",
        "    if not results:\n",
        "        print(\"No results generated.\")\n",
        "        return None\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "\n",
        "    df_sorted = df_res.sort_values(by=\"stress_probability\", ascending=False)\n",
        "\n",
        "    print(\"\\n--- Top 5 Most Stressed Windows ---\")\n",
        "    display(df_sorted.head(5))\n",
        "\n",
        "    most_stressed = df_sorted.iloc[0]\n",
        "    print(f\"\\nMost stressed window found: {most_stressed['start_time_s']}s - {most_stressed['end_time_s']}s\")\n",
        "    print(f\"Probability: {most_stressed['stress_probability']:.4f}\")\n",
        "\n",
        "    return df_res\n",
        "\n",
        "target_file = \"/content/Job Interview.mp3\"\n",
        "stress_df = analyze_stress_segments(target_file)"
      ],
      "metadata": {
        "id": "kSRUN_CYZDuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "def predict_stress_from_mp3(mp3_filepath):\n",
        "    df_opensmile = smile.process_file(mp3_filepath)\n",
        "    sorted_opensmile_features = newdf['feature_names']\n",
        "    feature_vector = [df_opensmile[col].iloc[0] for col in sorted_opensmile_features]\n",
        "    feature_array = np.array(feature_vector)\n",
        "\n",
        "    feature_tensor = torch.from_numpy(feature_array).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(feature_tensor)\n",
        "\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "    prediction = (probs > 0.5).int().item()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def predict_stress_sliding_window(mp3_filepath):\n",
        "    WINDOW_SIZE = 10000\n",
        "    STEP = 5000\n",
        "\n",
        "    temp_dir = \"temp_inference_segments\"\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "    os.makedirs(temp_dir)\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_mp3(mp3_filepath)\n",
        "\n",
        "        start = 0\n",
        "        clip_paths = []\n",
        "\n",
        "        filename = os.path.basename(mp3_filepath)\n",
        "        name, _ = os.path.splitext(filename)\n",
        "\n",
        "        while start + WINDOW_SIZE <= len(audio):\n",
        "            end = start + WINDOW_SIZE\n",
        "            subclip = audio[start:end]\n",
        "\n",
        "            out_name = f\"{name}_s{start//1000}_e{end//1000}.mp3\"\n",
        "            out_path = os.path.join(temp_dir, out_name)\n",
        "\n",
        "            subclip.export(out_path, format=\"mp3\")\n",
        "            clip_paths.append(out_path)\n",
        "\n",
        "            start += STEP\n",
        "\n",
        "        if not clip_paths:\n",
        "            print(f\"Audio file {mp3_filepath} is too short (< 10s) to extract segments.\")\n",
        "            return None\n",
        "\n",
        "        features_list = []\n",
        "        sorted_opensmile_features = newdf['feature_names']\n",
        "\n",
        "        print(f\"Processing {len(clip_paths)} segments...\")\n",
        "        for cp in clip_paths:\n",
        "            df_opensmile = smile.process_file(cp)\n",
        "\n",
        "            feature_vector = [df_opensmile[col].iloc[0] for col in sorted_opensmile_features]\n",
        "            features_list.append(feature_vector)\n",
        "\n",
        "        X_sequence = np.array(features_list)\n",
        "        X_tensor = torch.from_numpy(X_sequence).float().unsqueeze(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(X_tensor)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            prediction = (probs > 0.5).int().item()\n",
        "\n",
        "        return prediction, probs.item()\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "\n",
        "onesample = \"/content/drive/MyDrive/stressedjob3.mp3\"\n",
        "if os.path.exists(onesample):\n",
        "    pred, prob = predict_stress_sliding_window(onesample)\n",
        "    print(f\"\\nFile: {onesample}\")\n",
        "    print(f\"Prediction: {pred} (Stressed)\" if pred == 1 else f\"Prediction: {pred} (Not Stressed)\")\n",
        "    print(f\"Probability: {prob:.4f}\")\n",
        "else:\n",
        "    print(f\"Sample file not found: {onesample}\")"
      ],
      "metadata": {
        "id": "8E1_reE6ZXbr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}